{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b416e6c",
   "metadata": {},
   "source": [
    "# Normal likelihood examples\n",
    "\n",
    "Assume $$X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} \\text{Normal}(m, 1 / \\tau)$$\n",
    "\n",
    "The normal likelihood for one data point is\n",
    "$$\n",
    "f(x_i \\mid m, 1/\\tau^2) = \\sqrt{\\frac{\\tau}{2 \\pi}} \\exp\\left[ - \\frac{\\tau}{2}(x_i - m)^2 \\right]\n",
    "$$\n",
    "\n",
    "The likelihood for all the data is\n",
    "\n",
    "The normal likelihood for one data point is\n",
    "$$\n",
    "f(x_1, \\ldots, x_n \\mid m, 1/\\tau^2) = \\left(\\frac{\\tau}{2 \\pi}\\right)^{n/2} \\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - m)^2 \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "NB: evaluating the above function can get so close to $0$, that the computer will incorrectly round down to $0$. This is numerical underflow. It's bad, so to avoid it we'll work with the natural log of the above:\n",
    "\n",
    "$$\n",
    "\\log f(x_1, \\ldots, x_n \\mid m, 1/\\tau^2) = (n/2)\\log(\\tau) - (n/2) \\log (2 \\pi) - \\frac{\\tau}{2}\\sum_i (x_i - m)^2\n",
    "$$\n",
    "\n",
    "This isn't really an issue when you're dealing with conjugate priors, bt it will be when we aren't later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf72327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 1000\n",
    "x_data = np.random.normal(loc=0.0, scale=2.0, size=n)# pretend this data comes from real life and we don't know the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed78b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normal_log_like(data, mean, precision):\n",
    "#     n = data.shape[0]\n",
    "#     return .5*n*np.log(precision) - .5*n*np.log(2*np.pi) - .5*precision*np.sum((data-mean)**2)\n",
    "#normal_log_like(x_data, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8457888",
   "metadata": {},
   "source": [
    "### case 1: known precision/variance\n",
    "\n",
    "let's say we know the precision of the data: $\\tau = 3$\n",
    "\n",
    "then we only have to put a prior on $m$. The conjugate prior is normal.\n",
    "\n",
    "$$\n",
    "p(m) = \\text{Normal}(\\mu_0, 1/\\tau_0)\n",
    "$$\n",
    "\n",
    "we pick $\\mu_0$ and $\\tau_0$.\n",
    "\n",
    "#### Bayes rule:\n",
    "\n",
    "$$\n",
    "p(m \\mid x_1, \\ldots, x_n) \\propto p(x_1, \\ldots, x_n \\mid m)p(m) \n",
    "$$\n",
    "\n",
    "See example in slides and screenshotted derivation for proof:\n",
    "$$\n",
    "p(m \\mid x_1, \\ldots, x_n) = \\text{Normal}(\\mu_0 \\left(\\frac{\\tau_0}{\\tau_0 + n\\tau} \\right) + \\bar{x} \\left(\\frac{n \\tau}{\\tau_0 + n\\tau} \\right), [\\tau_0 + n\\tau]^{-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc98b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we use conjugate priors, we only need to use Python like a simple calculator\n",
    "xbar = np.average(x_data)\n",
    "tau0 = 0.001 # chosen hyperparameter\n",
    "mu0 = 0  # chosen hyperparameter\n",
    "tau = 3 # assumed known\n",
    "# \"getting\" the posterior is simply invoking the formula\n",
    "posterior_mean = xbar*(n*tau/(tau0 + n*tau )) + mu0*(tau0/(tau0 + n*tau ))\n",
    "posterior_precision = tau0 + n*tau \n",
    "posterior_variance = 1/posterior_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582d00",
   "metadata": {},
   "source": [
    "### case 2: unknown precision/variance\n",
    "\n",
    "let's say we don't know the precision/variance of the data. Then we have to have a prior for both $m$ and $\\tau$. \n",
    "\n",
    "(Carol, in the second screenshot you posted, they call $\\tau$ $w$. Let's stick with $\\tau$ for continuity, though.)\n",
    "\n",
    "The conjugate prior is\n",
    "\n",
    "$$\n",
    "p(m \\mid \\tau)p(\\tau) = \\text{Normal}(\\mu_0, \\frac{1}{\\tau\\tau_0})\\text{Gamma}(\\alpha_0/2, \\beta_0/2)\n",
    "$$\n",
    "\n",
    "we have to pick $\\mu_0$, $\\tau_0$, $\\alpha_0$, and $\\beta_0$. In Python, this will just be assigning four `float` variables.\n",
    "\n",
    "#### Bayes rule:\n",
    "\n",
    "We'll end up with a product posterior. Product one will be a normal, and product two will be a gamma. \n",
    "$$\n",
    "p(m, \\tau \\mid x_1, \\ldots, x_n) = p(m \\mid \\tau, x_1, \\ldots, x_n)p( \\tau \\mid x_1, \\ldots, x_n)\n",
    "$$\n",
    "\n",
    "Specifically\n",
    "$$\n",
    "p(m, \\tau \\mid x_1, \\ldots, x_n) = \\text{Normal}(m ; \\mu_0\\frac{\\tau_0}{n + \\tau_0} + \\bar{x}\\frac{n}{n + \\tau_0 }, \\tau^{-1}(n + \\tau_0)^{-1}) \\times \n",
    "\\text{Gamma}(\\tau ; (n+\\alpha_0)/2, \\frac{\\beta_0 + \\sum_i (x_i - \\bar{x})^2 + \\frac{ n \\tau_0}{(n + \\tau_0)} (\\bar{x} - \\mu_0)^2 }{2})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### things to notice:\n",
    "\n",
    "The overall/joint posterior $p(m, \\tau \\mid x_1, \\ldots, x_n)$ is not normal. It doesn't make sense to have a normal distribution for a thing that must be positive $\\tau$.\n",
    "\n",
    "The *conditional* posterior $p(m \\mid \\tau, x_1, \\ldots, x_n)$ is normal, but it depends on a thing you don't really know $(\\tau)$ \n",
    "\n",
    "The *conditional* posterior $p(m \\mid \\tau, x_1, \\ldots, x_n)$ has the same form as the above situation with known variance/precision.\n",
    "\n",
    "The *marginal* posterior $p(\\tau \\mid x_1, \\ldots, x_n)$ is Gamma. \n",
    "\n",
    "The four *prior* hyperparameter were $\\mu_0$, $\\tau_0$, $\\alpha_0$, and $\\beta_0$. \n",
    "\n",
    "These turned into four *posterior* hyperparameters $\\mu_0\\frac{\\tau_0}{n + \\tau_0} + \\bar{x}\\frac{n}{n + \\tau_0 }, (n + \\tau_0 )^{-1}, n+\\alpha_0, \\beta_0 + \\sum_i (x_i - \\bar{x})^2 + \\frac{ n \\tau_0}{(n + \\tau_0)} (\\bar{x} - \\mu_0)^2$ \n",
    "\n",
    "Sometimes they call the posterior hyper parameters with  $\\mu_n$, $\\tau_n$, $\\alpha_n$, and $\\beta_n$. This is shorter, but it doesn't tell you the update formulae.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f83f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick prior hyperparameters encoding prior knowledge about m, tau\n",
    "mu_0 = 0\n",
    "tau_0 = .01\n",
    "alpha_0 = 1\n",
    "beta_0 = 1\n",
    "# calculate posterior hyperparameters\n",
    "# I copy/pasted a lot of this from above\n",
    "xbar = np.mean(x_data)\n",
    "mu_n = xbar*(n/(tau_0 + n )) + mu_0*(tau_0/(tau_0 + n ))\n",
    "tau_n = n + tau_0\n",
    "alpha_n = n + alpha_0\n",
    "beta_n = beta_0 +  np.var(x_data)*n + (n*tau_0)/(n + tau_0)*(xbar-mu_0)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac78a7",
   "metadata": {},
   "source": [
    "#### How to pick prior\n",
    "\n",
    "When you don't know your variance, you need to pick $\\mu_0$, $\\tau_0$, $\\alpha_0$, and $\\beta_0$. \n",
    "\n",
    "If you want the prior expectation to be a certain number, you set $\\mu_0$ equal to it.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[m] = \\mathbb{E}[ \\mathbb{E}[ m \\mid \\tau ]] = \\mu_0\n",
    "$$\n",
    "\n",
    "One down three to go...\n",
    "\n",
    "$\\tau_0$ can be thought of an equivalent number of observations. So assume your brain has three data points for free. Then $\\tau_0 = 3$. \n",
    "\n",
    "Finally, we have to pick the last two. There are a range of alpha/beta pairs that would give you $P(M > 9)$. The easiest way is to simulate guess and check. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dcd4bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.219"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import gamma, norm\n",
    "\n",
    "def get_approx_prob_greater_than_nine(mu0, tau0, alpha0, beta0, num_samples):\n",
    "    \n",
    "    random_taus = gamma.rvs(a=alpha0,scale=1/beta0,size=num_samples)\n",
    "    random_mus = norm.rvs(loc=mu0, scale = 1/np.sqrt(random_taus*tau0), size=num_samples)\n",
    "    return np.average(random_mus > 9)\n",
    "\n",
    "get_approx_prob_greater_than_nine(3, 3, .1, .1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca337509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofs below "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e79541",
   "metadata": {},
   "source": [
    "Proof (feel free to ignore on a first reading)\n",
    "\n",
    "First find part 1/2 $p(m \\mid \\tau, x_1, \\ldots, x_n)$. We can ignore $\\tau$ pieces here.\n",
    "\n",
    "\\begin{align*}\n",
    "p(m \\mid \\tau, x_1, \\ldots, x_n)\n",
    "&\\propto \n",
    "f(x_1, \\ldots, x_n \\mid m, 1/\\tau^2) p(m \\mid \\tau)p(\\tau) \\\\\n",
    "&\\propto\n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - m)^2 \\right] \n",
    "\\exp\\left[ - \\frac{\\tau \\tau_0}{2}(m - \\mu_0)^2 \\right]\\\\\n",
    "&\\propto \n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - m)^2  - \\frac{\\tau \\tau_0}{2}(m - \\mu_0)^2 \\right] \\\\\n",
    "&\\propto \n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\left\\{ \\sum_i (x_i - m)^2  + \\tau_0(m - \\mu_0)^2\\right\\} \\right] \\\\\n",
    "&\\propto \n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\left\\{ nm^2 - 2m n \\bar{x}  + \\tau_0(m^2 - 2m \\mu_0)\\right\\} \\right] \\\\\n",
    "&\\propto \n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\left\\{ m^2 (n + \\tau_0) - 2m (\\mu_0\\tau_0 + n\\bar{x}) \\right\\} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "So\n",
    "$$\n",
    "m \\mid \\tau, x_1, \\ldots, x_n \\sim \\text{Normal}(\\mu_0\\frac{\\tau_0}{n + \\tau_0} + \\bar{x}\\frac{n}{n + \\tau_0 }, \\tau^{-1}(n + \\tau_0)^{-1})\n",
    "$$\n",
    "\n",
    "This is the same formula as the above. It depends on $\\tau$--we can do that because this is a *conditional* posterior. The mean is a weighted average, and the posterior precisions are additive. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d25eb8",
   "metadata": {},
   "source": [
    "Now let's find the second part--the marginal posterior $p( \\tau \\mid x_1, \\ldots, x_n)$. \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\tau \\mid x_1, \\ldots, x_n) \n",
    "&\\propto\n",
    "\\int\n",
    "p( x_1, \\ldots, x_n \\mid m, \\tau)p(m \\mid \\tau) p(\\tau) dm\n",
    "\\\\\n",
    "&=\n",
    "\\tau^{n/2}\\tau^{1/2}\\tau^{\\alpha_0/2-1}\\exp[\\tau \\beta_0/2]\n",
    "\\int\n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - m)^2 \\right] \n",
    "\\exp\\left[ - \\frac{\\tau \\tau_0}{2}(m - \\mu_0)^2 \\right]\n",
    "dm \\\\\n",
    "&=\n",
    "\\tau^{n/2}\\tau^{1/2}\\tau^{\\alpha_0/2-1}\\exp[\\tau \\beta_0/2]\n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - \\bar{x})^2 \\right] \n",
    "\\int\n",
    "\\exp\\left[ - \\frac{\\tau}{2}n (m - \\bar{x})^2 \\right] \n",
    "\\exp\\left[ - \\frac{\\tau \\tau_0}{2}(m - \\mu_0)^2 \\right]\n",
    "dm \\\\\n",
    "&=\n",
    "\\tau^{n/2}\\tau^{1/2}\\tau^{\\alpha_0/2-1}\\exp[\\tau \\beta_0/2]\n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - \\bar{x})^2 \\right] \n",
    "\\int\n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\left(n (m - \\bar{x})^2 + \\tau_0(m - \\mu_0)^2 \\right)  \\right] \n",
    "dm \\\\\n",
    "&=\n",
    "\\tau^{n/2}\\tau^{\\alpha_0/2-1}\\exp[\\tau \\beta_0/2]\n",
    "\\exp\\left[ - \\frac{\\tau}{2}\\sum_i (x_i - \\bar{x})^2 \\right] \n",
    "\\exp\\left[\\frac{ \\tau n \\tau_0}{2(n + \\tau_0} (\\bar{x} - \\mu_0)^2 \\right]\n",
    "\\end{align*}\n",
    "\n",
    "So the marginal posterior for $\\tau$ is \n",
    "\n",
    "$$\n",
    "\\text{Gamma}\\left(\n",
    "\\frac{n+\\alpha_0}{2}, \n",
    "\\frac{\\beta_0 + \\sum_i (x_i - \\bar{x})^2 + \\frac{ n \\tau_0}{(n + \\tau_0)} (\\bar{x} - \\mu_0)^2 }{2}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "The joint posterior is the product of these two functions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
