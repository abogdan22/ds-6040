{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b416e6c",
   "metadata": {},
   "source": [
    "# Multivariate Normal likelihood examples\n",
    "\n",
    "Assume $$X_1, \\ldots, X_n \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\Lambda^{-1})$$\n",
    "\n",
    "$\\mu$ is a mean $d \\times 1$ *vector* and $\\Lambda$ is a $d \\times d$ precision *matrix*. \n",
    "\n",
    "\n",
    "\n",
    "The normal likelihood for one data *vector* (probably a row in a data frame) is\n",
    "$$\n",
    "f(x_i \\mid \\mu, \\Lambda) = (2 \\pi)^{-d/2} \\det(\\Lambda)^{1/2} \\exp\\left[ - \\frac{1}{2}(x_i - \\mu)^\\intercal \\Lambda(x_i - \\mu) \\right]\n",
    "$$\n",
    "\n",
    "The likelihood $p(x_1, \\ldots, x_n \\mid \\mu, \\Lambda)$ is \n",
    "\n",
    "$$\n",
    "f(x_1, \\ldots, x_n \\mid \\mu, \\Lambda) = (2 \\pi)^{-nd/2} \\det(\\Lambda)^{n/2} \\exp\\left[ - \\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^\\intercal \\Lambda(x_i - \\mu) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NB: *evaluating* the above function can get so close to $0$, that the computer will incorrectly round down to $0$.  This is numerical underflow. It's bad, so to avoid it we'll work with the natural log of the above. \n",
    "\n",
    "This isn't really an issue when you're dealing with conjugate priors, bt it will be when we aren't later on. We don't require *evaluation* of the normal density yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf72327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.363376</td>\n",
       "      <td>0.550225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.133325</td>\n",
       "      <td>-1.527546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.280575</td>\n",
       "      <td>2.547358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.777862</td>\n",
       "      <td>-0.998420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493537</td>\n",
       "      <td>-2.375177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  2.363376  0.550225\n",
       "1 -0.133325 -1.527546\n",
       "2 -1.280575  2.547358\n",
       "3 -1.777862 -0.998420\n",
       "4  0.493537 -2.375177"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pretend this data comes from real life and we don't know the parameters\n",
    "n = 1000\n",
    "d = 2\n",
    "x_data = np.random.normal(loc=0.0, scale=2.0, size=(n,d))\n",
    "x_data = pd.DataFrame(x_data)\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8457888",
   "metadata": {},
   "source": [
    "### case 1: known precision matrix\n",
    "\n",
    "let's say we know the precision of the data: $\\Lambda = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "then we only have to put a prior on $\\mu$. The conjugate prior is normal.\n",
    "\n",
    "$$\n",
    "p(\\mu) = \\text{Normal}(\\mu_0, \\Lambda_0)\n",
    "$$\n",
    "\n",
    "we pick $\\mu_0$ and $\\Lambda_0$.\n",
    "\n",
    "#### Bayes rule:\n",
    "\n",
    "$$\n",
    "p(\\mu \\mid x_1, \\ldots, x_n) \\propto p(x_1, \\ldots, x_n \\mid \\mu)p(\\mu) \n",
    "$$\n",
    "\n",
    "which gives us this specific normal posterior:\n",
    "\n",
    "$$\n",
    "\\mu \\mid x_1, \\ldots, x_n \\sim \\text{Normal}\\left(\\mu_0\\left[\\frac{\\Lambda_0}{n\\Lambda + \\Lambda_0} \\right] + \\bar{x} \\left[\\frac{n\\Lambda}{n\\Lambda + \\Lambda_0} \\right],\n",
    "\\left[ n\\Lambda + \\Lambda_0\\right]^{-1}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This is very similar to the univariate case 1 in the other document. We just replace univariate precisions with matrix precisions. Computationally, remember to use matrix multiplication (`@`) instead of elementwise multiplication (`*`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc98b5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bayesian mean estimate:  [-0.00803584  0.09730708]\n",
      "frequentist mean estimate [-0.00804388  0.09740438]\n"
     ]
    }
   ],
   "source": [
    "# when we use conjugate priors, we only need to use Python like a simple calculator\n",
    "xbar = x_data.mean().values\n",
    "Lambda0 = np.array([[1,0],[0,1]]) # chosen hyperparameter\n",
    "mu0 = np.array([0,0])  # chosen hyperparameter\n",
    "Lambda = np.array([[1,0],[0,1]]) # assumed known\n",
    "# \"getting\" the posterior is simply invoking the formula\n",
    "posterior_mean = xbar@(n*Lambda) @ np.linalg.inv(Lambda0 + n*Lambda ) + mu0@Lambda0@ np.linalg.inv(Lambda0 + n*Lambda )\n",
    "posterior_precision = Lambda0 + n*Lambda \n",
    "posterior_covariance_matrix = np.linalg.inv(posterior_precision)\n",
    "print(\"bayesian mean estimate: \", posterior_mean)\n",
    "print(\"frequentist mean estimate\", xbar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582d00",
   "metadata": {},
   "source": [
    "### case 2: unknown precision/variance\n",
    "\n",
    "let's say we don't know the precision matrix of the data $\\Lambda$. Then we have to have a prior for both $\\mu$ and $\\Lambda$. Everything that's unknown goes into the prior.\n",
    "\n",
    "The conjugate prior is \n",
    "\n",
    "$$\n",
    "p(\\mu \\mid \\Lambda)p(\\Lambda) = \\text{Normal}(\\mu_0, \\frac{1}{\\kappa_0}\\Lambda^{-1})\\text{Wishart}(\\Lambda_0, \\nu_0)\n",
    "$$\n",
    "\n",
    "Notice that\n",
    "\n",
    "- a \"Wishart\" is kind of like a multivariate \"Gamma\". Gamma is for positive random variables, Wishart is for \"positive-definite\" random matrices.\n",
    "- $\\mu$ and $\\Lambda$ are the unknown parameters\n",
    "- $\\mu_0, \\kappa_0, \\Lambda_0$ and $\\nu_0$ are chosen by us\n",
    "- I like to simulate $\\mu$ and $\\Lambda$ after I choose prior hyperparameters just to make sure it looks like the values look sensible\n",
    "- Additionally, $E[\\Lambda] = \\Lambda_0 \\nu_0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce4ee02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.04366694, 2.51588385]),\n",
       " array([[2.18887717, 1.4885916 ],\n",
       "        [1.4885916 , 2.18117816]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Picking the prior\n",
    "\"\"\"\n",
    "\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# chosen prior hyperparameters\n",
    "mu_0 = np.array([2,3])\n",
    "kappa_0 = 3\n",
    "Lambda_0 = np.array([[1,0],[0,1]])\n",
    "nu_0 = 2\n",
    "\n",
    "# simulate parameters from the prior\n",
    "def sim_prior(mu_0, kappa_0, Lambda_0, nu_0):\n",
    "    # sim Lambda\n",
    "    Lambda = wishart.rvs(df = nu_0, scale = Lambda_0, size = 1)\n",
    "    # sim mu given Lambda\n",
    "    mu = multivariate_normal.rvs(mean=mu_0, cov=np.linalg.inv(Lambda), size=1)\n",
    "    return mu, Lambda\n",
    "\n",
    "# do these look like reasonable values of mu and Lambda?\n",
    "sim_prior(mu_0, kappa_0, Lambda_0, nu_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d16edb",
   "metadata": {},
   "source": [
    "#### Getting the posterior\n",
    "\n",
    "We'll end up with a product posterior. Product one will be a multivariate normal, and product two will be a Wishart. It's a conjugate prior, so the prior and posterior are the same distribution.\n",
    "\n",
    "$$\n",
    "p(\\mu, \\Lambda \\mid x_1, \\ldots, x_n) = p(\\mu \\mid \\Lambda, x_1, \\ldots, x_n)p( \\Lambda \\mid x_1, \\ldots, x_n)\n",
    "$$\n",
    "\n",
    "#### things to notice:\n",
    "\n",
    "The overall/joint posterior $p(\\mu, \\Lambda \\mid x_1, \\ldots, x_n) $ is not normal. It doesn't make sense to have a normal distribution for a thing that must be \"positive\" $\\Lambda$.\n",
    "\n",
    "The *conditional* posterior $p(\\mu \\mid \\Lambda, x_1, \\ldots, x_n)$ is normal, but it depends on a thing you don't really know $\\Lambda$ \n",
    "\n",
    "The *conditional* posterior $p(\\mu \\mid \\Lambda, x_1, \\ldots, x_n)$  has the same form as the above situation with known variance/precision.\n",
    "\n",
    "The *marginal* posterior $p(\\Lambda \\mid x_1, \\ldots, x_n)$ is Wishart. \n",
    "\n",
    "The four *prior* hyperparameters are $\\mu_0, \\kappa_0, \\Lambda_0$ and $\\nu_0$\n",
    "\n",
    "These turned into four *posterior* hyperparameters $\\frac{n}{n + \\kappa_0} \\bar{x} + \\frac{ \\kappa_0}{n + \\kappa_0} \\mu_0$, $n + \\kappa_0$, $\\left( \n",
    "\\Lambda_0^{-1} +\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "+\n",
    "\\frac{n\\kappa_0}{n + \\kappa_0} \n",
    "\\left(\\bar{x} -  \\mu_0\\right)\n",
    "\\left(\\bar{x} -  \\mu_0\\right)^\\intercal \n",
    "\\right)^{-1}$ and $n + \\nu_0$\n",
    "\n",
    "Sometimes they call the posterior hyper parameters with  $\\mu_n, \\kappa_n, \\Lambda_n$ and $\\nu_n$. This is shorter, but it doesn't tell you the update formulae.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f83f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian posterior mean:  [-0.00203777  0.10608613]\n",
      "Frequentist mean estimate:  [-0.00804388  0.09740438]\n"
     ]
    }
   ],
   "source": [
    "# we picked prior hyperparameters in the previous cell\n",
    "# calculate posterior hyperparameters\n",
    "mu_n = xbar * (n/(kappa_0 + n )) + mu_0*(kappa_0/(kappa_0 + n ))\n",
    "kappa_n = n + kappa_0\n",
    "nu_n = n + nu_0\n",
    "Lambda_n_inv = Lambda_0 + (x_data - xbar).T @ (x_data - xbar) + (n*kappa_0)/(n+kappa_0)*np.outer(xbar - mu_0, xbar - mu_0)\n",
    "Lambda_n = np.linalg.inv(Lambda_n_inv)\n",
    "\n",
    "# if you pick noninformative priors, they should be roughly the same\n",
    "print(\"Bayesian posterior mean: \", mu_n)\n",
    "print(\"Frequentist mean estimate: \", xbar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca337509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofs below are commented out...you can uncomment them to see them if you're interested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587f7bf",
   "metadata": {},
   "source": [
    "<!-- ### Proof of posterior for unknown precision matrix case (case 2)\n",
    "\n",
    "\n",
    "\n",
    "First find part 1/2 conditional posterior $p(\\mu \\mid \\Lambda, x_1, \\ldots, x_n)$. We can ignore $\\Lambda$ pieces here.\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mu \\mid \\Lambda, x_1, \\ldots, x_n)\n",
    "&\\propto \n",
    "f(x_1, \\ldots, x_n \\mid \\mu, \\Lambda) p(\\mu \\mid \\Lambda)p(\\Lambda) \\\\\n",
    "&\\propto \n",
    "f(x_1, \\ldots, x_n \\mid \\mu, \\Lambda^{-1}) p(\\mu \\mid \\Lambda) \\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{1}{2}\\sum_i (x_i - \\mu)^\\intercal\\Lambda(x_i- \\mu) \\right)\n",
    "\\exp\\left[-\\frac{\\kappa_0}{2} (\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0)\\right]\\\\\n",
    "&=\n",
    "\\exp\\left(-\\frac{1}{2}\n",
    "\\left\\{\n",
    "\\sum_i (\\mu - x_i)^\\intercal\\Lambda(\\mu - x_i)\n",
    "+\n",
    "\\kappa_0(\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0)\n",
    "\\right\\}\n",
    " \\right)\n",
    "\\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{1}{2}\n",
    "\\left\\{\n",
    "n \\mu^\\intercal \\Lambda \\mu \n",
    "-2n \\bar{x}^\\intercal\\Lambda \\mu \n",
    "+\n",
    "\\kappa_0(\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0)\n",
    "\\right\\}\n",
    " \\right)\n",
    "\\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{1}{2}\n",
    "\\left\\{\n",
    "n \\mu^\\intercal \\Lambda \\mu \n",
    "-2n \\bar{x}^\\intercal\\Lambda \\mu \n",
    "+\n",
    "\\kappa_0 \\mu^\\intercal  \\Lambda \\mu\n",
    "- 2 \\kappa_0 \\mu_0^\\intercal \\Lambda \\mu\n",
    "\\right\\}\n",
    " \\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(-\\frac{1}{2}\n",
    "\\left\\{\n",
    "\\mu^\\intercal\\left[ (n + \\kappa_0) \\Lambda \\right] \\mu \n",
    "-2\\left[ n \\bar{x} + \\kappa_0 \\mu_0 \\right]^\\intercal\\Lambda \\mu \n",
    "\\right\\}\n",
    " \\right)\n",
    "\\\\\n",
    "&=\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2} (\\mu - \\left[ (n + \\kappa_0) \\Lambda\\right]^{-1} \\left[ n \\bar{x} + \\kappa_0 \\mu_0 \\right])^\\intercal  \\Lambda\\left[ (n + \\kappa_0) \\Lambda\\right] (\\mu - \\left[ (n + \\kappa_0) \\Lambda\\right]^{-1} \\left[ n \\bar{x} + \\kappa_0 \\mu_0 \\right]\\Lambda)\n",
    "\\right)\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So\n",
    "$$\n",
    "\\mu \\mid \\Lambda, x_1, \\ldots, x_n \\sim \\text{Normal}(\n",
    "\\frac{n}{n + \\kappa_0} \\bar{x} + \\frac{ \\kappa_0}{n + \\kappa_0} \\mu_0, \n",
    "\\left[ (n + \\kappa_0) \\Lambda\\right]^{-1}\n",
    ")\n",
    "$$\n",
    "\n",
    "\n",
    "Second find part 2/2 the marginal posterior $p(\\Lambda \\mid x_1, \\ldots, x_n)$. \n",
    "\n",
    "In this part of the proof, we use the *trace operator* (which takes a matrix and returns the sum of its diagonal elements). The trace operator is used to write down Wishart densities, and our goal is to organize stuff into a Wishart density.\n",
    "\n",
    "\n",
    "Now\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\Lambda \\mid x_1, \\ldots, x_n) \n",
    "&\\propto\n",
    "\\int\n",
    "p( x_1, \\ldots, x_n \\mid \\mu, \\Lambda)p(\\mu \\mid \\Lambda) p(\\Lambda) d\\mu\n",
    "\\\\\n",
    "&\\propto\n",
    "\\int\n",
    "\\det(\\Lambda)^{n/2} \n",
    "\\exp\\left(-\\frac{1}{2}\\sum_i (x_i - \\mu)^\\intercal\\Lambda(x_i- \\mu) \\right) \\times \\\\\n",
    "&\\hspace{10mm} \n",
    "\\det(\\Lambda)^{1/2}\n",
    "\\exp\\left[-\\frac{\\kappa_0}{2} (\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0)\\right] \\times \\\\\n",
    "&\\hspace{10mm} \n",
    "\\det(\\Lambda)^{(\\nu_0 - d - 1)/2}\n",
    "\\exp\\left[-\\frac{1}{2} \\text{tr} \\left( \\Lambda_0^{-1}\\Lambda \\right) \\right]\n",
    "d\\mu \\\\\n",
    "&=\n",
    "\\det(\\Lambda)^{(n + \\nu_0 - d)/2} \n",
    "\\exp\\left[-\\frac{1}{2} \\text{tr} \\left( \\Lambda_0^{-1}\\Lambda \\right) \\right]\\times \\\\\n",
    "&\\hspace{10mm} \n",
    "\\int \\exp\\left(-\\frac{1}{2}\\sum_i (x_i - \\mu)^\\intercal\\Lambda(x_i- \\mu) \\right) \n",
    "\\exp\\left[-\\frac{\\kappa_0}{2} (\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0)\\right] d\\mu\n",
    "\\end{align*}\n",
    "\n",
    "We can try to \"recognize\" a normal distirbution in that last integral. That will allow us to use the formula for a Gaussian density instead of doing the wrote integration. To clean up the notation, let's look at negative twice the logarithm of the exponential part and play with that. Remember, don't drop anything that involves $\\Lambda$.\n",
    "\n",
    "\\begin{align*}\n",
    "&\\sum_i (x_i - \\mu)^\\intercal\\Lambda(x_i- \\mu)\n",
    "+\n",
    "\\kappa_0 (\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0) \\\\\n",
    "&=\n",
    "\\sum_i x_i^\\intercal\\Lambda x_i\n",
    "- 2 n \\bar{x}^\\intercal \\Lambda \\mu\n",
    "+ n\\mu^\\intercal \\Lambda \\mu\n",
    "+ \\kappa_0 \\mu^\\intercal \\Lambda \\mu\n",
    "- 2 \\kappa_0 \\mu_0^\\intercal  \\Lambda \\mu \n",
    "+ \\kappa_0 \\mu_0^\\intercal \\Lambda \\mu_0 \\\\\n",
    "&=\n",
    "(n + \\kappa_0) \\mu^\\intercal \\Lambda \\mu\n",
    "- 2 \\left[ n \\bar{x} +  \\kappa_0 \\mu_0 \\right]^\\intercal\\Lambda \\mu\n",
    "+ \\underbrace{\\sum_i x_i^\\intercal\\Lambda x_i\n",
    "+\n",
    "\\kappa_0 \\mu_0^\\intercal \\Lambda \\mu_0 }_{\\text{no $\\mu$}} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\sum_i x_i^\\intercal\\Lambda x_i\n",
    "+\n",
    "\\kappa_0 \\mu_0^\\intercal \\Lambda \\mu_0 \n",
    "-\n",
    "\\frac{1}{n + \\kappa_0}\n",
    "\\{ n \\bar{x} +  \\kappa_0 \\mu_0 \\}^\\intercal\n",
    "\\Lambda \n",
    "\\{ n \\bar{x} +  \\kappa_0 \\mu_0 \\} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\sum_i x_i^\\intercal\\Lambda x_i\n",
    "+\n",
    "\\kappa_0 \\mu_0^\\intercal \\Lambda \\mu_0 \n",
    "-\n",
    "\\frac{1}{n + \\kappa_0}\n",
    "\\left\\{\n",
    "n^2 \\bar{x}^\\intercal \\Lambda \\bar{x}\n",
    "+ 2 n \\bar{x}^\\intercal \\Lambda \\kappa_0 \\mu_0\n",
    "+  \\kappa_0^2 \\mu_0^\\intercal \\Lambda \\mu_0\n",
    "\\right\\} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\text{tr} \\left\\{\\left[\n",
    "\\sum_i x_i x_i^\\intercal\n",
    "+\n",
    "\\kappa_0 \\mu_0 \\mu_0^\\intercal \n",
    "-\n",
    "\\frac{n^2 }{n + \\kappa_0} \\bar{x} \\bar{x}^\\intercal \n",
    "- 2\\frac{n \\kappa_0}{n + \\kappa_0}   \\mu_0 \\bar{x}^\\intercal\n",
    "-  \\frac{\\kappa_0^2}{n + \\kappa_0}  \\mu_0 \\mu_0^\\intercal \n",
    "\\right]\n",
    "\\Lambda \\right\\} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\text{tr} \\left\\{\\left[\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "+\n",
    "\\kappa_0 \\mu_0 \\mu_0^\\intercal \n",
    "-\n",
    "\\frac{n^2 - n(n + \\kappa_0)}{n + \\kappa_0} \\bar{x} \\bar{x}^\\intercal \n",
    "- 2\\frac{n \\kappa_0}{n + \\kappa_0}   \\mu_0 \\bar{x}^\\intercal\n",
    "-  \\frac{\\kappa_0^2}{n + \\kappa_0}  \\mu_0 \\mu_0^\\intercal \n",
    "\\right]\n",
    "\\Lambda \\right\\} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\text{tr} \\left\\{\\left[\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "-\n",
    "\\frac{n^2 - n(n + \\kappa_0)}{n + \\kappa_0} \\bar{x} \\bar{x}^\\intercal \n",
    "- 2\\frac{n \\kappa_0}{n + \\kappa_0}   \\mu_0 \\bar{x}^\\intercal\n",
    "-  \\frac{\\kappa_0^2 - \\kappa_0(n + \\kappa_0)}{n + \\kappa_0}  \\mu_0 \\mu_0^\\intercal \n",
    "\\right]\n",
    "\\Lambda \\right\\} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\text{tr} \\left\\{\\left[\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "+\n",
    "\\frac{n\\kappa_0}{n + \\kappa_0} \\bar{x} \\bar{x}^\\intercal \n",
    "- 2\\frac{n \\kappa_0}{n + \\kappa_0}   \\mu_0 \\bar{x}^\\intercal\n",
    "+   \\frac{n \\kappa_0}{n + \\kappa_0}  \\mu_0 \\mu_0^\\intercal \n",
    "\\right]\n",
    "\\Lambda \\right\\} \\\\\n",
    "&=\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]^\\intercal\n",
    "\\left[ (n + \\kappa_0)\\Lambda \\right]\n",
    "\\left[\\mu - \\{ \\frac{n}{n + \\kappa_0} \\bar{x} +  \\frac{\\kappa_0}{n + \\kappa_0} \\mu_0 \\} \\right]\n",
    "+\\text{tr} \\left\\{\\left[\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "+\n",
    "\\frac{n\\kappa_0}{n + \\kappa_0} \n",
    "\\left(\\bar{x} -  \\mu_0\\right)\n",
    "\\left(\\bar{x} -  \\mu_0\\right)^\\intercal \\right]\n",
    "\\Lambda \\right\\} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "So we can finish it off now\n",
    "\n",
    "\\begin{align*}\n",
    "&\\det(\\Lambda)^{(n + \\nu_0 - d)/2} \n",
    "\\exp\\left[-\\frac{1}{2} \\text{tr} \\left( \\Lambda_0^{-1}\\Lambda \\right) \\right]\\times \\\\\n",
    "&\\hspace{10mm} \n",
    "\\int \\exp\\left(-\\frac{1}{2}\\sum_i (x_i - \\mu)^\\intercal\\Lambda(x_i- \\mu) \\right) \n",
    "\\exp\\left[-\\frac{\\kappa_0}{2} (\\mu - \\mu_0)^\\intercal  \\Lambda (\\mu - \\mu_0)\\right] d\\mu \\\\\n",
    "&\\propto\n",
    "\\det(\\Lambda)^{(n + \\nu_0 - d - 1)/2} \n",
    "\\exp\\left[\n",
    "-\\frac{1}{2} \\text{tr}\n",
    "\\left\\{\n",
    "\\Lambda_0^{-1}\n",
    "+\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "+\n",
    "\\frac{n\\kappa_0}{n + \\kappa_0} \n",
    "\\left(\\bar{x} -  \\mu_0\\right)\n",
    "\\left(\\bar{x} -  \\mu_0\\right)^\\intercal \n",
    "\\right\\}\\Lambda \n",
    "\\right]\n",
    "\\end{align*}\n",
    "\n",
    "This means\n",
    "\n",
    "$$\n",
    "\\Lambda \\mid x_1, \\ldots, x_n \\sim \\text{Wishart}\\left(n + \\nu_0, \\left( \n",
    "\\Lambda_0^{-1} +\n",
    "\\sum_i (x_i - \\bar{x}) (x_i - \\bar{x})^\\intercal\n",
    "+\n",
    "\\frac{n\\kappa_0}{n + \\kappa_0} \n",
    "\\left(\\bar{x} -  \\mu_0\\right)\n",
    "\\left(\\bar{x} -  \\mu_0\\right)^\\intercal \n",
    "\\right)^{-1}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fde443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofs in cell above ^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
